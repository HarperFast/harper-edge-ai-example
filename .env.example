# ============================================
# Ollama Configuration
# ============================================
# HTTP endpoint for Ollama server
OLLAMA_HOST=http://localhost:11434
# Default LLM model to use with Ollama
OLLAMA_DEFAULT_MODEL=llama2

# ============================================
# Inference Engine Configuration
# ============================================
# Maximum number of models to keep in LRU cache
MODEL_CACHE_SIZE=10

# ============================================
# Debug Mode
# ============================================
# Enable debug logging throughout the application
DEBUG=false
