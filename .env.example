# ============================================
# Harper Configuration
# ============================================
# Harper instance URL for CLI tools
# HARPER_URL=http://localhost:9926
# CLI_TARGET_URL=http://localhost:9926  # Alternative name

# ============================================
# Deployment Configuration (for deploy.sh)
# ============================================
# Remote Harper instance for deployment
# For local development, use: ./deploy.sh --local (targets http://localhost:9926)
# DEPLOY_REMOTE_HOST=ai-ops.irjudson-ai.harperfabric.com
# DEPLOY_REMOTE_PORT=9925
# DEPLOY_REMOTE_URL=https://ai-ops.irjudson-ai.harperfabric.com:9925

# Harper admin credentials for deployment
# DEPLOY_USERNAME=HDB_ADMIN
# DEPLOY_PASSWORD=your-password

# Deploy options
# DEPLOY_REPLICATED=true        # Replicate across cluster nodes
# DEPLOY_RESTART=true            # Restart after deploy (true/false/rolling)

# Backend selection for deployment (skips interactive prompt)
# Options: all, onnx, tensorflow, transformers (comma-separated)
# Note: Ollama backend is always available (0 MB - external service)
# Examples:
#   DEPLOY_BACKENDS=all                    # All backends (911MB - WARNING: >800MB)
#   DEPLOY_BACKENDS=onnx                   # ONNX only (183MB)
#   DEPLOY_BACKENDS=onnx,transformers      # ONNX + Transformers (228MB)
#   DEPLOY_BACKENDS=tensorflow             # TensorFlow only (683MB)
# Leave empty for interactive selection
# DEPLOY_BACKENDS=

# ============================================
# Ollama Configuration
# ============================================
# HTTP endpoint for Ollama server
OLLAMA_HOST=http://localhost:11434
# Default LLM model to use with Ollama
OLLAMA_DEFAULT_MODEL=llama2

# ============================================
# Inference Engine Configuration
# ============================================
# Maximum number of models to keep in LRU cache
MODEL_CACHE_SIZE=10

# ============================================
# Model Fetch System
# ============================================
# Optional shared token for Model Fetch API authentication
# If not set, authentication is disabled (open access)
# MODEL_FETCH_TOKEN=your-secret-token-here

# Enable/disable background worker for model fetching
MODEL_FETCH_WORKER_ENABLED=true

# Maximum concurrent fetch jobs
MODEL_FETCH_MAX_CONCURRENT=3

# Worker poll interval in milliseconds
MODEL_FETCH_POLL_INTERVAL=5000

# Maximum file size for model downloads (bytes)
MODEL_FETCH_MAX_FILE_SIZE=5368709120

# Maximum retry attempts for failed jobs
MODEL_FETCH_MAX_RETRIES=3

# Initial retry delay in milliseconds
MODEL_FETCH_INITIAL_RETRY_DELAY=5000

# ============================================
# Debug Mode
# ============================================
# Enable debug logging throughout the application
DEBUG=false
