# ============================================
# Ollama Configuration
# ============================================
# HTTP endpoint for Ollama server
OLLAMA_HOST=http://localhost:11434
# Default LLM model to use with Ollama
OLLAMA_DEFAULT_MODEL=llama2

# ============================================
# Inference Engine Configuration
# ============================================
# Maximum number of models to keep in LRU cache
MODEL_CACHE_SIZE=10

# ============================================
# Model Fetch System
# ============================================
# Optional shared token for Model Fetch API authentication
# If not set, authentication is disabled (open access)
# MODEL_FETCH_TOKEN=your-secret-token-here

# Enable/disable background worker for model fetching
MODEL_FETCH_WORKER_ENABLED=true

# Maximum concurrent fetch jobs
MODEL_FETCH_MAX_CONCURRENT=3

# Worker poll interval in milliseconds
MODEL_FETCH_POLL_INTERVAL=5000

# Maximum file size for model downloads (bytes)
MODEL_FETCH_MAX_FILE_SIZE=5368709120

# Maximum retry attempts for failed jobs
MODEL_FETCH_MAX_RETRIES=3

# Initial retry delay in milliseconds
MODEL_FETCH_INITIAL_RETRY_DELAY=5000

# ============================================
# Debug Mode
# ============================================
# Enable debug logging throughout the application
DEBUG=false
