# Harper Edge AI MLOps Platform - Design Document

**Date**: 2025-11-24
**Status**: Draft
**Authors**: Design collaboration with Claude Code

## Executive Summary

This design extends the current Harper edge AI example into a production-grade MLOps platform with **full component pluggability**. The architecture spans three deployment tiers with each component designed as a pluggable interface, allowing customers to use Harper-native implementations or integrate external services (Vertex AI, BigQuery, Snowflake, etc.).

**Core Philosophy**: Works standalone with Harper-native components, extensible with external integrations.

**Integration Strategy**: Hybrid approach

- **Harper-native components**: Code-level interfaces (TypeScript/JavaScript) for in-process efficiency
- **External integrations**: Protocol-level interfaces (REST, gRPC, GraphQL) for cloud services
- **Interoperability**: Standard data formats and contracts across all components

## Architecture Overview

The platform consists of three main deployment tiers:

### 1. Production Edge Inferencing (Distributed edge deployments)

**Not a cluster** - a collection of independent edge deployments running at:

- Web applications (client-side or server-side)
- IoT devices and sensors
- Mobile applications
- Regional data centers
- CDN edge nodes

**Pluggable Components**:

- **Device Communication**: MQTT, Pub/Sub, WebSockets, REST
- **Inference Engine**: TensorFlow.js, ONNX Runtime, Vertex AI endpoints
- **Monitoring Backend**: Default (Harper), Datadog, Prometheus, Vertex AI Monitoring
- **Local Data Store**: Harper, SQLite, in-memory cache
- **Message Bus**: Harper Plexus (node-to-node), MQTT/Pub/Sub (device-to-edge)

**Characteristics**:

- Autonomous operation (works offline)
- Low-latency inference (<50ms)
- Telemetry back to AIOps cluster

### 2. Harper AIOps Cluster (Centralized MLOps control plane)

**Pluggable Components**:

- **Data Sources**: Default (Harper landing zone), BigQuery, Snowflake, S3
- **Feature Store**: Default (Harper tables), Feast, Vertex Feature Store, Tecton
- **Training Provider**: Default (Harper-local TF.js), Vertex AI Pipelines, SageMaker, Azure ML
- **Model Registry**: Default (Harper tables + filesystem), MLflow, Vertex AI Model Registry
- **Model Repository**: Default (Harper-hosted), HuggingFace, TensorFlow Hub, private registries
- **CI/CD Pipeline**: Default (Harper-native), GitHub Actions, Cloud Build, Jenkins
- **Cluster Message Bus**: Harper Plexus, Pub/Sub, Kafka, RabbitMQ
- **Feature Engineering**: Default (Harper transforms), Dataflow, Spark, DBT

### 3. Legacy Enterprise & Public Cloud Infrastructure (Integration layer)

External systems that integrate with the Harper AIOps cluster:

- **Enterprise data warehouses**: BigQuery, Snowflake, Redshift
- **Legacy databases**: PostgreSQL, Oracle, SQL Server
- **Data lakes**: S3, Azure Data Lake, GCS
- **Public AI services**: Vertex AI, SageMaker, Azure ML
- **Pre-trained model repositories**: HuggingFace, TensorFlow Hub
- **Cloud messaging**: Pub/Sub, Event Hubs, SNS/SQS

## Core Component Interfaces

This section defines the pluggable interfaces for each component type. Each interface uses the **hybrid approach**: Harper-native implementations use code-level interfaces (TypeScript), while external integrations use protocol-level APIs (REST/gRPC).

### 1. Inference Engine Interface

**Purpose**: Execute model predictions on input features

**Code Interface** (Harper-native):

```typescript
interface InferenceEngine {
	// Initialize and load model
	initialize(modelConfig: ModelConfig): Promise<void>;

	// Perform inference
	predict(features: FeatureVector): Promise<Prediction>;

	// Batch prediction for efficiency
	predictBatch(features: FeatureVector[]): Promise<Prediction[]>;

	// Health and metrics
	getStatus(): EngineStatus;
	getMetrics(): InferenceMetrics;

	// Cleanup
	dispose(): Promise<void>;
}

interface ModelConfig {
	modelId: string;
	modelVersion: string;
	format: 'tensorflow' | 'onnx' | 'custom';
	path?: string; // Local path for edge models
	endpoint?: string; // Remote endpoint for cloud models
	timeout?: number; // For cloud inference
	fallback?: ModelConfig; // For dual inference pattern
}
```

**Protocol Interface** (External services):

```
POST /v1/models/{model_id}/predict
Content-Type: application/json

Request:
{
  "instances": [<feature_vector>],
  "version": "optional_version"
}

Response:
{
  "predictions": [<prediction>],
  "metadata": {
    "model_id": "...",
    "model_version": "...",
    "latency_ms": 45
  }
}
```

**Reference Implementations**:

- **Default** (TensorFlow.js - current implementation for local edge inference)
- `ONNXRuntimeEngine` (planned - faster edge inference)
- `VertexAIEngine` (future - cloud inference with timeout)

---

### 2. Feature Store Interface

**Purpose**: Store, retrieve, and serve ML features for training and inference

**Code Interface** (Harper-native):

```typescript
interface FeatureStore {
	// Write features
	writeFeatures(entityId: string, features: FeatureMap, timestamp?: Date): Promise<void>;
	writeFeaturesAsync(entityId: string, features: FeatureMap): void;

	// Read features for inference (latest)
	getFeatures(entityId: string, featureNames: string[]): Promise<FeatureMap>;

	// Read features for training (point-in-time)
	getFeaturesAsOf(entityId: string, featureNames: string[], timestamp: Date): Promise<FeatureMap>;

	// Batch operations
	getFeaturesBatch(entityIds: string[], featureNames: string[]): Promise<Map<string, FeatureMap>>;

	// Metadata
	getFeatureMetadata(featureName: string): Promise<FeatureMetadata>;
	listFeatures(): Promise<string[]>;
}

interface FeatureMap {
	[featureName: string]: number | string | number[] | boolean;
}

interface FeatureMetadata {
	name: string;
	type: 'numeric' | 'categorical' | 'embedding' | 'boolean';
	description?: string;
	statistics?: {
		min?: number;
		max?: number;
		mean?: number;
		categories?: string[];
	};
}
```

**Protocol Interface** (External feature stores like Vertex AI):

```
POST /v1/projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entity}/features:read
Content-Type: application/json

Request:
{
  "entity_id": "user_123",
  "feature_selector": {
    "id_matcher": {
      "ids": ["feature_1", "feature_2"]
    }
  }
}

Response:
{
  "entity_view": {
    "entity_id": "user_123",
    "data": {
      "feature_1": {"value": 0.85},
      "feature_2": {"value": "category_a"}
    }
  }
}
```

**Reference Implementations**:

- **Default** (Harper tables with time-series indexing)
- `VertexFeatureStoreAdapter` (future - bridges to Vertex AI Feature Store)
- `FeastAdapter` (future - integrates with Feast)

---

### 3. Training Provider Interface

**Purpose**: Train ML models from features and data

**Code Interface** (Harper-native):

```typescript
interface TrainingProvider {
	// Start training job
	train(trainingConfig: TrainingConfig): Promise<TrainingJob>;

	// Monitor training progress
	getJobStatus(jobId: string): Promise<JobStatus>;

	// Retrieve trained model
	getTrainedModel(jobId: string): Promise<TrainedModel>;

	// Cancel training
	cancelJob(jobId: string): Promise<void>;

	// List recent jobs
	listJobs(limit?: number): Promise<TrainingJob[]>;
}

interface TrainingConfig {
	modelType: 'classification' | 'regression' | 'ranking' | 'custom';
	framework: 'tensorflow' | 'pytorch' | 'xgboost' | 'custom';

	// Data configuration
	trainingData: DataSource;
	validationData?: DataSource;
	featureColumns: string[];
	labelColumn: string;

	// Hyperparameters
	hyperparameters: {
		learningRate?: number;
		epochs?: number;
		batchSize?: number;
		[key: string]: any;
	};

	// Output configuration
	outputPath: string;
	experimentName?: string;
	tags?: Record<string, string>;
}

interface TrainingJob {
	jobId: string;
	status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';
	startTime?: Date;
	endTime?: Date;
	metrics?: TrainingMetrics;
	modelArtifact?: string; // Path or URI to trained model
}

interface TrainingMetrics {
	loss?: number;
	accuracy?: number;
	[metricName: string]: number | undefined;
}
```

**Protocol Interface** (External services like Vertex AI):

```
POST /v1/projects/{project}/locations/{location}/trainingJobs
Content-Type: application/json

Request:
{
  "display_name": "product-recommendation-v2",
  "training_task_inputs": {
    "model_type": "CLASSIFICATION",
    "training_data_uri": "gs://bucket/training-data",
    "validation_data_uri": "gs://bucket/validation-data",
    "hyperparameters": {
      "learning_rate": 0.001,
      "epochs": 100
    }
  }
}

Response:
{
  "name": "projects/.../trainingJobs/12345",
  "state": "TRAINING_RUNNING",
  "create_time": "2025-01-24T10:00:00Z"
}
```

**Reference Implementations**:

- **Default** (trains models in Harper cluster using TensorFlow.js Node)
- `VertexAIPipelinesAdapter` (future - triggers Vertex AI training pipelines)
- `SageMakerAdapter` (future - uses Amazon SageMaker)

---

### 4. Model Registry Interface

**Purpose**: Version, store, and manage model metadata and artifacts

**Code Interface** (Harper-native):

```typescript
interface ModelRegistry {
	// Register new model
	registerModel(model: ModelRegistration): Promise<RegisteredModel>;

	// Retrieve model metadata
	getModel(modelId: string, version?: string): Promise<RegisteredModel>;

	// List models
	listModels(filter?: ModelFilter): Promise<RegisteredModel[]>;

	// Update model metadata
	updateModel(modelId: string, updates: Partial<ModelMetadata>): Promise<void>;

	// Model lifecycle
	promoteModel(modelId: string, version: string, stage: ModelStage): Promise<void>;
	archiveModel(modelId: string, version: string): Promise<void>;

	// Download model artifact
	downloadModel(modelId: string, version: string): Promise<Buffer | ReadableStream>;
}

interface ModelRegistration {
	modelId: string;
	version: string;
	framework: string;
	artifactUri: string; // Where model files are stored
	metadata: ModelMetadata;
}

interface RegisteredModel {
	modelId: string;
	version: string;
	framework: string;
	artifactUri: string;
	stage: ModelStage;
	metrics?: Record<string, number>;
	metadata: ModelMetadata;
	registeredAt: Date;
	registeredBy?: string;
}

type ModelStage = 'development' | 'staging' | 'production' | 'archived';

interface ModelMetadata {
	description?: string;
	tags?: Record<string, string>;
	inputSchema?: object;
	outputSchema?: object;
	trainingJobId?: string;
	parentModelId?: string; // For retraining lineage
}

interface ModelFilter {
	stage?: ModelStage;
	framework?: string;
	tags?: Record<string, string>;
	limit?: number;
}
```

**Protocol Interface** (External registries):

```
POST /v1/projects/{project}/locations/{location}/models
Content-Type: application/json

Request:
{
  "display_name": "product-recommender-v2.1",
  "version_id": "2.1.0",
  "artifact_uri": "gs://models/product-recommender/v2.1",
  "metadata": {
    "framework": "tensorflow",
    "metrics": {"accuracy": 0.92}
  }
}

Response:
{
  "name": "projects/.../models/12345",
  "version_id": "2.1.0",
  "create_time": "2025-01-24T10:00:00Z"
}
```

**Reference Implementations**:

- **Default** (stores metadata in Harper tables, artifacts in filesystem/S3)
- `MLflowAdapter` (future - integrates with MLflow Model Registry)
- `VertexModelRegistryAdapter` (future - syncs with Vertex AI Model Registry)

---

### 5. Message Bus Interface

**Purpose**: Pub/sub messaging for events, telemetry, and data streaming

**Code Interface** (Harper-native):

```typescript
interface MessageBus {
	// Publish messages
	publish(topic: string, message: Message): Promise<void>;
	publishBatch(topic: string, messages: Message[]): Promise<void>;

	// Subscribe to topics
	subscribe(topic: string, handler: MessageHandler): Subscription;

	// Topic management
	createTopic(topic: string, config?: TopicConfig): Promise<void>;
	deleteTopic(topic: string): Promise<void>;
	listTopics(): Promise<string[]>;

	// Health
	getStatus(): MessageBusStatus;
}

interface Message {
	id?: string;
	data: any; // Payload
	attributes?: Record<string, string>; // Metadata
	timestamp?: Date;
}

type MessageHandler = (message: Message) => Promise<void> | void;

interface Subscription {
	unsubscribe(): void;
	pause(): void;
	resume(): void;
}

interface TopicConfig {
	retention?: number; // Retention period in milliseconds
	ordering?: boolean; // Maintain message order
	replication?: boolean; // Cross-cluster replication
}

interface MessageBusStatus {
	connected: boolean;
	topicCount: number;
	subscriptionCount: number;
}
```

**Protocol Interface** (External services like Pub/Sub):

```
POST /v1/projects/{project}/topics/{topic}:publish
Content-Type: application/json

Request:
{
  "messages": [
    {
      "data": "<base64-encoded-payload>",
      "attributes": {
        "event_type": "model_deployed",
        "model_id": "prod-v2.1"
      }
    }
  ]
}

Response:
{
  "messageIds": ["12345"]
}

---

POST /v1/projects/{project}/subscriptions/{subscription}:pull
Content-Type: application/json

Request:
{
  "maxMessages": 10
}

Response:
{
  "receivedMessages": [
    {
      "ackId": "...",
      "message": {
        "data": "...",
        "attributes": {...}
      }
    }
  ]
}
```

**Reference Implementations**:

- **Default** (uses Harper's built-in Plexus replication/pub-sub)
- `PubSubAdapter` (future - bridges to Google Cloud Pub/Sub)
- `KafkaAdapter` (future - integrates with Apache Kafka)
- `MQTTAdapter` (edge devices - lightweight pub/sub for IoT)

---

### 6. Data Source Interface

**Purpose**: Ingest raw data from various sources into the AIOps cluster landing zone

**Code Interface** (Harper-native):

```typescript
interface DataSource {
	// Connect to data source
	connect(config: DataSourceConfig): Promise<void>;

	// Read data
	read(query: DataQuery): Promise<DataStream>;
	readBatch(query: DataQuery): Promise<DataBatch>;

	// Write data (for bidirectional sources)
	write(data: DataBatch): Promise<void>;

	// Schema discovery
	getSchema(): Promise<DataSchema>;

	// Connection management
	disconnect(): Promise<void>;
	getStatus(): ConnectionStatus;
}

interface DataSourceConfig {
	type: 'bigquery' | 'snowflake' | 'postgres' | 's3' | 'harper' | 'custom';
	connection: {
		host?: string;
		credentials?: any;
		database?: string;
		[key: string]: any;
	};
}

interface DataQuery {
	table?: string;
	sql?: string;
	filters?: Record<string, any>;
	limit?: number;
	offset?: number;
}

interface DataStream {
	// Streaming interface for large datasets
	read(): AsyncIterableIterator<Record<string, any>>;
	close(): void;
}

interface DataBatch {
	rows: Record<string, any>[];
	schema: DataSchema;
	metadata?: {
		rowCount: number;
		byteSize: number;
	};
}

interface DataSchema {
	columns: ColumnSchema[];
}

interface ColumnSchema {
	name: string;
	type: 'string' | 'number' | 'boolean' | 'date' | 'json' | 'array';
	nullable: boolean;
	description?: string;
}

interface ConnectionStatus {
	connected: boolean;
	lastSync?: Date;
	errorMessage?: string;
}
```

**Protocol Interface** (External sources):

```
// BigQuery example
POST https://bigquery.googleapis.com/bigquery/v2/projects/{project}/queries
Content-Type: application/json

Request:
{
  "query": "SELECT * FROM dataset.table WHERE timestamp > @start_time",
  "parameters": [
    {"name": "start_time", "value": "2025-01-24"}
  ]
}

Response:
{
  "rows": [...],
  "schema": {...},
  "totalRows": "1000000"
}
```

**Reference Implementations**:

- **Default** (read from Harper tables)
- `BigQueryDataSource` (common - enterprise data warehouse)
- `SnowflakeDataSource` (future - another popular warehouse)
- `PostgresDataSource` (future - legacy databases)
- `S3DataSource` (future - data lake integration)

---

### 7. Monitoring Backend Interface

**Purpose**: Collect, store, and query inference metrics and drift signals

**Code Interface** (Harper-native):

```typescript
interface MonitoringBackend {
	// Record inference events
	recordInference(event: InferenceEvent): Promise<void>;
	recordInferenceBatch(events: InferenceEvent[]): Promise<void>;

	// Record metrics
	recordMetric(metric: Metric): Promise<void>;

	// Query metrics
	queryMetrics(query: MetricQuery): Promise<MetricResult[]>;

	// Drift detection
	detectDrift(config: DriftConfig): Promise<DriftResult>;

	// Alerts
	createAlert(alert: AlertConfig): Promise<string>;
	listAlerts(): Promise<Alert[]>;
}

interface InferenceEvent {
	timestamp: Date;
	modelId: string;
	modelVersion: string;
	requestId: string;

	// Inference details
	features: Record<string, any>;
	prediction: any;
	confidence?: number;
	latencyMs: number;

	// Context
	edgeLocation?: string;
	deviceId?: string;
	userId?: string;

	// Optional ground truth for performance tracking
	actualLabel?: any;
}

interface Metric {
	name: string;
	value: number;
	timestamp: Date;
	tags?: Record<string, string>;
}

interface MetricQuery {
	metricName: string;
	startTime: Date;
	endTime: Date;
	aggregation?: 'avg' | 'sum' | 'min' | 'max' | 'p50' | 'p95' | 'p99';
	groupBy?: string[];
	filters?: Record<string, string>;
}

interface MetricResult {
	timestamp: Date;
	value: number;
	tags?: Record<string, string>;
}

interface DriftConfig {
	modelId: string;
	modelVersion: string;
	featureNames?: string[]; // Specific features to check, or all
	startTime: Date;
	endTime: Date;
	baselineWindow?: {
		// Compare against baseline period
		startTime: Date;
		endTime: Date;
	};
}

interface DriftResult {
	overallDrift: number; // 0-1 score, higher = more drift
	featureDrift: {
		[featureName: string]: {
			score: number;
			method: 'psi' | 'kl_divergence' | 'ks_test';
		};
	};
	recommendation: 'no_action' | 'investigate' | 'retrain';
}

interface AlertConfig {
	name: string;
	condition: string; // e.g., "avg_confidence < 0.7"
	window: number; // Time window in milliseconds
	action: 'notify' | 'trigger_retrain' | 'webhook';
	actionConfig?: any;
}

interface Alert {
	id: string;
	config: AlertConfig;
	status: 'active' | 'triggered' | 'resolved';
	triggeredAt?: Date;
}
```

**Protocol Interface** (External monitoring services):

```
// Prometheus-style metrics endpoint
GET /metrics

Response:
# HELP model_inference_latency_seconds Inference latency
# TYPE model_inference_latency_seconds histogram
model_inference_latency_seconds_bucket{model="prod-v2",le="0.01"} 850
model_inference_latency_seconds_bucket{model="prod-v2",le="0.05"} 920
model_inference_latency_seconds_sum{model="prod-v2"} 45.2
model_inference_latency_seconds_count{model="prod-v2"} 1000

---

// Custom JSON API for external services
POST /v1/monitoring/events
Content-Type: application/json

Request:
{
  "events": [
    {
      "timestamp": "2025-01-24T10:00:00Z",
      "model_id": "prod-v2",
      "prediction": {...},
      "latency_ms": 42
    }
  ]
}
```

**Reference Implementations**:

- **Default** (stores events in Harper time-series tables)
- `VertexAIMonitoringAdapter` (future - integrates with Vertex AI Model Monitoring)
- `PrometheusAdapter` (future - exports metrics to Prometheus)
- `DatadogAdapter` (future - sends to Datadog APM)

---

## Data Flows

### Flow A: Training Pipeline (Bottom-Up)

This flow describes how raw data becomes production-ready models deployed to edge locations.

#### Step 1: Data Ingestion

```
External Sources → Data Source → Harper AIOps Landing Zone
```

- External sources: BigQuery, Snowflake, legacy databases, public datasets
- Data Source interface reads from external systems
- Raw data lands in Harper tables (Data Sources component)
- Can be triggered by:
  - Scheduled ETL jobs (daily, hourly, etc.)
  - Event-driven (new data available webhook)
  - Manual data upload

#### Step 2: Feature Engineering

```
Data Sources → Feature Engineering → Feature Store
```

- Read raw data from landing zone
- Transform into ML features:
  - Aggregations (user stats, product metrics)
  - Derived features (ratios, embeddings, time-based)
  - Categorical encoding, normalization
- Write features to Feature Store with:
  - Entity ID (user_id, product_id, etc.)
  - Feature vector
  - Timestamp (for point-in-time correctness)
- Feature validation and schema enforcement

#### Step 3: Model Training

```
Feature Store → Training Provider → Training Job
```

- Training Provider reads features for training set
- Configure training:
  - Model type (classification, regression, ranking)
  - Framework (TensorFlow, PyTorch, XGBoost)
  - Hyperparameters
  - Train/validation split
- Training Provider orchestrates:
  - **Default**: Runs training in Harper cluster (TensorFlow.js Node)
  - **External**: Triggers Vertex AI Pipeline, SageMaker job
- Produces training metrics during execution

#### Step 4: Model Validation & Testing

```
Training Job → Model Validation → Pass/Fail Decision
```

- Evaluate model on validation set
- Compute metrics:
  - Accuracy, precision, recall, AUC
  - Business metrics (revenue impact, latency)
- Quality gates:
  - Metrics exceed thresholds
  - No significant bias detected
  - Performance acceptable on edge devices
- Automated or manual approval required

#### Step 5: Model Registration

```
Validated Model → Model Registry
```

- Register model with metadata:
  - Model ID and version
  - Framework and format
  - Training metrics
  - Training job lineage
  - Artifact URI (where model files stored)
- Initial stage: `development`
- Tag with: experiment name, data version, git commit

#### Step 6: Model Promotion & Deployment

```
Model Registry → CI/CD Pipeline → Edge Locations
```

- Promote model through stages:
  - `development` → `staging` → `production`
- CI/CD Pipeline orchestrates deployment:
  - Package model artifact
  - Generate deployment manifest
  - Push to edge locations via:
    - **Default**: Harper replication (Plexus)
    - **External**: Cloud storage + pull mechanism
- Deployment strategies:
  - Canary: 5% of edge nodes first
  - Blue/Green: Parallel deployment with cutover
  - Progressive rollout: Region by region
- Edge locations pull new model and reload Inference Engine

#### Step 7: Production Serving

```
Edge Locations → Production Inference
```

- Edge deployments now serving with new model
- Monitoring begins collecting metrics
- Feedback loop starts (Flow B)

---

### Flow B: Inference & Feedback Loop (Circular)

This flow describes real-time inference at the edge, monitoring for degradation, and triggering retraining when needed.

#### Step 1: Device/Client Request

```
Device/Client → Edge Location (REST/WebSocket/MQTT)
```

- Client sends request via supported protocols:
  - **REST**: HTTP POST to `/predict` endpoint
  - **WebSocket**: Real-time bidirectional connection
  - **MQTT**: Lightweight pub/sub for IoT devices
- Request includes:
  - User context (activity type, location, preferences)
  - Entity IDs (user_id, session_id)
  - Raw input data

#### Step 2: Feature Retrieval

```
Inference Service → Feature Store → Features
```

- Look up pre-computed features for entity IDs
- Retrieve latest features from local Feature Store
- Combine with request context
- Feature vector ready for inference

#### Step 3: Model Inference

```
Features → Inference Engine → Prediction
```

- Inference Engine (TensorFlow.js) executes model
- Input: Feature vector
- Output: Prediction + confidence score
- Latency: 30-50ms (local edge inference)
- Track: Request ID, timestamp, latency

#### Step 4: Response to Client

```
Prediction → Client
```

- Return prediction with metadata:
  - Prediction result
  - Confidence score
  - Model version used
  - Request ID for tracking
- Client uses prediction in application logic

#### Step 5: Telemetry Collection

```
Inference Service → Monitoring Backend
```

- Record inference event asynchronously (non-blocking):
  - Request ID, timestamp
  - Model ID and version
  - Features used (or sample)
  - Prediction and confidence
  - Latency, edge location, device ID
  - Ground truth label (if available later)
- Write to local Monitoring Backend
- Batch events for efficiency

#### Step 6: Telemetry Aggregation

```
Edge Locations → Message Bus → AIOps Cluster Monitoring
```

- Edge monitoring backends publish telemetry to Message Bus
- Topics organized by event type:
  - `inference-events` - Individual predictions
  - `inference-metrics` - Aggregated metrics
  - `drift-signals` - Anomaly detections
- AIOps cluster subscribes and aggregates across all edges
- Provides centralized visibility

#### Step 7: Drift Detection

```
Monitoring Backend → Drift Detection → Drift Result
```

- Continuous analysis of inference telemetry:
  - **Input Drift**: Feature distributions shifting
    - Method: PSI (Population Stability Index), KS-test
    - Compare current window to training baseline
  - **Output Drift**: Prediction patterns changing
    - Track confidence distribution
    - Monitor prediction class balance
  - **Performance Degradation**: Accuracy dropping (if ground truth available)
    - Track actual vs predicted
    - Business metric trends

- Drift scoring:
  - Calculate per-feature drift scores
  - Aggregate into overall drift signal
  - Classify severity: `no_action`, `investigate`, `retrain`

#### Step 8: Retraining Decision

```
Drift Result → Retraining Trigger
```

- Automated trigger conditions:
  - Overall drift score > threshold (e.g., 0.7)
  - Confidence < threshold for X% of requests
  - Performance metric degradation > Y%
- Manual trigger options:
  - Operator reviews drift dashboard
  - Business event (new product launch, seasonal change)
- Decision modes:
  - **Automatic**: Trigger retraining immediately
  - **Manual Approval**: Notify operator, wait for confirmation
  - **Scheduled**: Queue for next training window

#### Step 9: Trigger Training Pipeline

```
Retraining Trigger → Message Bus → Training Provider
```

- Publish retraining event to Message Bus
- Event includes:
  - Reason: drift detected, performance degradation, manual
  - Current model ID and version
  - Drift analysis results
  - Suggested training configuration
- Training Provider subscribes and initiates Flow A:
  - Fetch latest data from Data Sources
  - Engineer fresh features
  - Train new model version
  - Validate and register
  - Deploy to edge locations

#### Step 10: Continuous Monitoring

```
New Model Deployed → Edge Inference → Monitoring (Loop)
```

- New model version deployed to edge
- Inference continues with updated model
- Monitoring tracks new model performance
- Compare against previous version (A/B if canary deployment)
- Feedback loop continues indefinitely

---

#### Key Characteristics of Flow B

**Asynchronous**: Inference and monitoring are decoupled - telemetry doesn't block predictions

**Resilient**: Edge locations operate autonomously even if AIOps cluster is unreachable

**Adaptive**: System automatically responds to changing data patterns

**Observable**: Full visibility into model behavior across all edge locations

---

## Monitoring & Drift Detection Implementation

This section details how the Monitoring Backend implements drift detection and retraining triggers, moving from **Phase 1 (C)** to **Phase 2 (D)** capabilities.

### Phase 1: Performance-Based + Drift Detection (Initial Implementation)

#### 1. Data Collection

```typescript
// Inference events stored in Harper time-series tables
interface InferenceEventRecord {
	timestamp: Date;
	requestId: string;
	modelId: string;
	modelVersion: string;
	edgeLocation: string;

	// Input features (sampled to control storage)
	features: Record<string, number | string>;

	// Output
	prediction: any;
	confidence: number;
	latencyMs: number;

	// Optional ground truth (added later if available)
	actualLabel?: any;
	feedbackTimestamp?: Date;
}
```

**Storage Strategy**:

- Full events: Last 7 days (for detailed analysis)
- Aggregated metrics: 90 days (for trend analysis)
- Statistical summaries: 1 year (for seasonal patterns)

#### 2. Input Drift Detection

**Method: Population Stability Index (PSI)**

- Compare feature distributions between baseline and current period
- Baseline: Training data distribution (stored during model registration)
- Current: Recent inference data (sliding window, e.g., last 24 hours)

```typescript
function calculatePSI(baseline: number[], current: number[]): number {
	// Bin both distributions into 10 buckets
	const bins = 10;
	const baselineBuckets = binDistribution(baseline, bins);
	const currentBuckets = binDistribution(current, bins);

	let psi = 0;
	for (let i = 0; i < bins; i++) {
		const baselinePct = baselineBuckets[i] / baseline.length;
		const currentPct = currentBuckets[i] / current.length;

		// Avoid division by zero
		if (baselinePct > 0 && currentPct > 0) {
			psi += (currentPct - baselinePct) * Math.log(currentPct / baselinePct);
		}
	}

	return psi;
}

// Interpretation:
// PSI < 0.1: No significant drift
// PSI 0.1-0.2: Moderate drift - investigate
// PSI > 0.2: High drift - retrain recommended
```

**Per-Feature Analysis**:

```typescript
interface FeatureDriftAnalysis {
	featureName: string;
	psiScore: number;
	status: 'stable' | 'moderate' | 'high';
	baselineStats: {
		mean: number;
		std: number;
		min: number;
		max: number;
	};
	currentStats: {
		mean: number;
		std: number;
		min: number;
		max: number;
	};
}
```

#### 3. Output Drift Detection

**Confidence Distribution Monitoring**:

- Track distribution of confidence scores over time
- Alert if:
  - Mean confidence drops below threshold (e.g., < 0.7)
  - Variance increases significantly (model uncertainty growing)
  - Percentage of low-confidence predictions (< 0.5) exceeds limit

```typescript
interface ConfidenceDriftMetrics {
	window: TimeWindow;
	meanConfidence: number;
	confidenceVariance: number;
	lowConfidencePct: number; // Predictions with confidence < 0.5
	confidenceTrend: 'stable' | 'declining' | 'improving';
}
```

**Prediction Distribution Shifts**:

- For classification: Monitor class balance changes
- For regression: Track prediction mean/variance

```typescript
// Classification example
interface PredictionDistribution {
	className: string;
	baselineFrequency: number; // From training data
	currentFrequency: number; // Recent predictions
	shift: number; // Difference
}

// Alert if any class shifts by > 20%
```

#### 4. Performance Degradation Detection (if ground truth available)

```typescript
interface PerformanceMetrics {
	window: TimeWindow;

	// Classification metrics
	accuracy?: number;
	precision?: Record<string, number>; // Per class
	recall?: Record<string, number>;
	f1Score?: number;

	// Regression metrics
	mse?: number;
	mae?: number;
	rmse?: number;

	// Business metrics (custom)
	businessMetric?: number;

	// Trend
	trend: 'improving' | 'stable' | 'degrading';
	degradationPct?: number; // % drop from baseline
}
```

**Ground Truth Collection**:

- Async feedback loop for collecting labels:
  - User interactions (click, purchase, rating)
  - Delayed outcomes (conversion, churn)
  - Human labeling workflows
- Join predictions with ground truth by `requestId`
- Calculate performance on recent labeled data

#### 5. Composite Drift Score & Retraining Decision

```typescript
interface DriftAssessment {
	timestamp: Date;
	modelId: string;
	modelVersion: string;

	// Component scores
	inputDriftScore: number; // 0-1, from PSI aggregation
	outputDriftScore: number; // 0-1, from confidence/distribution
	performanceDegradation: number; // 0-1, from accuracy drop

	// Overall assessment
	overallDriftScore: number; // Weighted combination
	recommendation: 'no_action' | 'investigate' | 'retrain';

	// Details
	driftingFeatures: FeatureDriftAnalysis[];
	confidenceMetrics: ConfidenceDriftMetrics;
	performanceMetrics?: PerformanceMetrics;
}

function assessDrift(
	inputDrift: FeatureDriftAnalysis[],
	confidenceMetrics: ConfidenceDriftMetrics,
	performanceMetrics?: PerformanceMetrics
): DriftAssessment {
	// Weight different signals
	const weights = {
		input: 0.4,
		output: 0.3,
		performance: 0.3,
	};

	// Calculate component scores
	const inputScore = Math.max(...inputDrift.map((f) => f.psiScore)) / 0.2; // Normalize to 0-1
	const outputScore = 1 - confidenceMetrics.meanConfidence;
	const perfScore = performanceMetrics?.degradationPct || 0;

	// Weighted overall score
	const overallScore = weights.input * inputScore + weights.output * outputScore + weights.performance * perfScore;

	// Decision thresholds
	let recommendation: 'no_action' | 'investigate' | 'retrain';
	if (overallScore > 0.7) {
		recommendation = 'retrain';
	} else if (overallScore > 0.4) {
		recommendation = 'investigate';
	} else {
		recommendation = 'no_action';
	}

	return {
		timestamp: new Date(),
		modelId: '...',
		modelVersion: '...',
		inputDriftScore: inputScore,
		outputDriftScore: outputScore,
		performanceDegradation: perfScore,
		overallDriftScore: overallScore,
		recommendation,
		driftingFeatures: inputDrift,
		confidenceMetrics,
		performanceMetrics,
	};
}
```

#### 6. Retraining Trigger Configuration

```typescript
interface RetrainingConfig {
	mode: 'automatic' | 'manual_approval' | 'disabled';

	// Trigger conditions (all configurable)
	triggers: {
		driftThreshold: number; // Default: 0.7
		confidenceThreshold: number; // Default: 0.7
		performanceDropPct: number; // Default: 0.15 (15% drop)
		minSampleSize: number; // Require N samples before triggering
		cooldownPeriod: number; // Min time between retrains (ms)
	};

	// Training configuration to use
	trainingTemplate: TrainingConfig;

	// Notification settings
	notifications: {
		email?: string[];
		slack?: string;
		webhook?: string;
	};
}
```

**Automatic Retraining Flow**:

```typescript
async function checkAndTriggerRetraining(driftAssessment: DriftAssessment, config: RetrainingConfig): Promise<void> {
	// Check if retraining needed
	if (driftAssessment.recommendation !== 'retrain') {
		return;
	}

	// Check cooldown period
	const lastRetrain = await getLastRetrainingTime(driftAssessment.modelId);
	if (Date.now() - lastRetrain.getTime() < config.triggers.cooldownPeriod) {
		console.log('Retraining skipped: cooldown period active');
		return;
	}

	// Mode check
	if (config.mode === 'disabled') {
		return;
	}

	if (config.mode === 'manual_approval') {
		// Send notification for human approval
		await sendRetrainingNotification(driftAssessment, config.notifications);
		return;
	}

	// Automatic mode: trigger retraining
	console.log('Triggering automatic retraining');

	// Publish retraining event to message bus
	await messageBus.publish('retraining-triggers', {
		reason: 'drift_detected',
		modelId: driftAssessment.modelId,
		currentVersion: driftAssessment.modelVersion,
		driftScore: driftAssessment.overallDriftScore,
		driftDetails: driftAssessment,
		trainingConfig: config.trainingTemplate,
	});

	// Record retraining event
	await recordRetrainingTrigger(driftAssessment);
}
```

---

### Phase 2: Full MLOps Observability Suite (Future Enhancement)

**Additional Capabilities** (designed for but not initially implemented):

#### 1. Concept Drift Detection

- Monitor relationship between features and predictions
- Detect when feature→prediction mappings change
- Use model explanation techniques (SHAP values) to track feature importance shifts

#### 2. Data Quality Monitoring

- Missing values, nulls, out-of-range values
- Schema violations
- Cardinality changes (new categories appearing)

#### 3. A/B Testing & Champion/Challenger

- Deploy multiple model versions simultaneously
- Split traffic between versions
- Statistical significance testing
- Automatic promotion of better-performing challenger

#### 4. Model Explainability Tracking

- Store explanation data (SHAP, LIME) for sample predictions
- Monitor explanation stability over time
- Alert if explanations become inconsistent

#### 5. Bias & Fairness Monitoring

- Track performance across demographic segments
- Detect disparate impact
- Fairness metrics (equal opportunity, demographic parity)

#### 6. Advanced Alerting

- Anomaly detection on metric time series
- Forecasting-based alerts (predict drift before it happens)
- Multi-condition alerts with complex logic

---

## Walking Skeleton: Minimal End-to-End Implementation

**Objective**: Build the simplest possible version that touches every component and demonstrates the complete Flow A (Training Pipeline) and Flow B (Inference & Feedback Loop).

**Philosophy**:

- Implement the thinnest slice through every layer
- Use the simplest possible implementation for each component
- Focus on integration over features
- Test-driven: Write tests first, implement to pass

### Walking Skeleton Scope

**What Gets Implemented**:

1. **Feature Store** (Minimal)
   - Single Harper table: `features`
   - Schema: `entity_id, feature_json, timestamp`
   - Two methods: `writeFeatures()`, `getFeatures()`
   - No point-in-time support yet (just latest)

2. **Training Provider** (Minimal)
   - Single model type: Binary classification
   - Framework: TensorFlow.js (existing)
   - Synchronous training (no job queue)
   - Fixed hyperparameters (no configuration yet)
   - Returns trained model in memory

3. **Model Registry** (Minimal)
   - Single Harper table: `models`
   - Schema: `model_id, version, artifact_path, metrics_json, created_at`
   - Two methods: `registerModel()`, `getModel()`
   - No staging/promotion yet

4. **Inference Engine** (Existing + Extensions)
   - ✅ Already works with TensorFlow.js
   - Add: Load model from Model Registry by ID
   - Add: Track request metadata for monitoring

5. **Monitoring Backend** (Minimal)
   - Single Harper table: `inference_events`
   - Schema: `request_id, timestamp, model_version, confidence, latency_ms`
   - One method: `recordInference()`
   - No drift detection yet (just data collection)

6. **Message Bus** (Minimal)
   - Use Harper Plexus directly
   - Single topic: `system-events`
   - Pub/sub for retraining triggers only
   - No topic management yet

7. **Deployment** (Minimal)
   - Manual: Copy model artifact to edge location
   - Edge reloads model on `/reload` endpoint
   - No CI/CD pipeline yet

---

### Walking Skeleton User Story

**As a data scientist**, I want to:

1. Upload training data to Harper
2. Train a simple binary classifier
3. Deploy it to an edge location
4. Make predictions via REST API
5. See inference telemetry being collected
6. Trigger a manual retrain

**Acceptance Criteria**:

```bash
# 1. Upload training data
curl -X POST http://localhost:9926/data/upload \
  -H "Content-Type: application/json" \
  -d '{"rows": [{"features": {"x": 1.0, "y": 2.0}, "label": 1}, ...]}'

# 2. Train model
curl -X POST http://localhost:9926/train \
  -H "Content-Type: application/json" \
  -d '{"model_id": "test-classifier", "label_column": "label"}'
# Response: {"model_id": "test-classifier", "version": "v1", "accuracy": 0.85}

# 3. Deploy model (happens automatically in walking skeleton)
# Model is now in registry and inference engine loads it

# 4. Make prediction
curl -X POST http://localhost:9926/predict \
  -H "Content-Type: application/json" \
  -d '{"entity_id": "user_123", "features": {"x": 1.5, "y": 2.5}}'
# Response: {"prediction": 1, "confidence": 0.92, "model_version": "v1"}

# 5. View telemetry
curl http://localhost:9926/monitoring/recent
# Response: [{"request_id": "...", "confidence": 0.92, "latency_ms": 45}, ...]

# 6. Trigger retrain (manual)
curl -X POST http://localhost:9926/retrain \
  -d '{"model_id": "test-classifier"}'
# Response: {"status": "training_started", "new_version": "v2"}
```

---

### Walking Skeleton Architecture

```
┌─────────────────────────────────────────────────────────┐
│                   Harper Instance                        │
│                                                          │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐ │
│  │   Tables:    │  │   Tables:    │  │   Tables:    │ │
│  │  - features  │  │  - models    │  │  - inference │ │
│  │  - raw_data  │  │              │  │    _events   │ │
│  └──────────────┘  └──────────────┘  └──────────────┘ │
│         ▲                 ▲                  ▲          │
│         │                 │                  │          │
│  ┌──────┴─────────────────┴──────────────────┴──────┐ │
│  │              REST API Resources                   │ │
│  │                                                    │ │
│  │  /data/upload  /train  /predict  /monitoring/*   │ │
│  │  /retrain      /models                            │ │
│  └───────────────────────────────────────────────────┘ │
│                                                          │
│  ┌───────────────────────────────────────────────────┐ │
│  │            Core Components (In-Process)           │ │
│  │                                                    │ │
│  │  FeatureStore → TrainingProvider → ModelRegistry │ │
│  │       ↓                                      ↓     │ │
│  │  InferenceEngine ← ← ← ← ← ← ← ← ← ← ← ← ← ┘     │ │
│  │       ↓                                            │ │
│  │  MonitoringBackend                                │ │
│  └───────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────┘

External Client (curl, web app, etc.)
        ↕
    REST API
```

**Key Simplifications**:

- Everything runs in a single Harper instance (no distributed deployment yet)
- All components are in-process (no separate services)
- Synchronous operations (no job queuing)
- Manual deployment (no CI/CD automation)
- No drift detection (just data collection)

---

### Walking Skeleton Test Plan (TDD Approach)

#### Test 1: Feature Store Read/Write

```typescript
describe('FeatureStore', () => {
	it('should write and read features for an entity', async () => {
		const store = new FeatureStore();
		await store.writeFeatures('user_123', { age: 25, city: 'SF' });
		const features = await store.getFeatures('user_123', ['age', 'city']);
		expect(features.age).toBe(25);
		expect(features.city).toBe('SF');
	});
});
```

#### Test 2: Training Pipeline

```typescript
describe('TrainingProvider', () => {
	it('should train a model and register it', async () => {
		// Setup: Create training data
		const data = [
			{ features: { x: 1, y: 2 }, label: 0 },
			{ features: { x: 2, y: 3 }, label: 1 },
		];

		// Train
		const trainer = new TrainingProvider();
		const job = await trainer.train({
			modelId: 'test-model',
			trainingData: data,
			featureColumns: ['x', 'y'],
			labelColumn: 'label',
		});

		// Verify job completed
		expect(job.status).toBe('completed');
		expect(job.metrics.accuracy).toBeGreaterThan(0.5);

		// Verify model registered
		const registry = new ModelRegistry();
		const model = await registry.getModel('test-model', job.modelVersion);
		expect(model).toBeDefined();
	});
});
```

#### Test 3: Inference with Monitoring

```typescript
describe('Inference Pipeline', () => {
	it('should make predictions and record telemetry', async () => {
		// Setup: Train and register a model
		const modelId = 'test-model';
		const modelVersion = 'v1';
		// ... training code ...

		// Load model into inference engine
		const engine = new InferenceEngine();
		await engine.initialize({ modelId, modelVersion });

		// Make prediction
		const prediction = await engine.predict({ x: 1.5, y: 2.5 });
		expect(prediction.prediction).toBeDefined();
		expect(prediction.confidence).toBeGreaterThan(0);

		// Verify telemetry recorded
		const monitoring = new MonitoringBackend();
		const events = await monitoring.queryEvents({
			modelId,
			startTime: new Date(Date.now() - 1000),
		});
		expect(events.length).toBe(1);
		expect(events[0].confidence).toBe(prediction.confidence);
	});
});
```

#### Test 4: End-to-End Flow

```typescript
describe('End-to-End MLOps Flow', () => {
	it('should complete full training, deployment, and inference cycle', async () => {
		// 1. Upload data
		const data = generateTrainingData(100);
		await uploadData(data);

		// 2. Train model
		const trainResult = await trainModel({ modelId: 'e2e-test' });
		expect(trainResult.status).toBe('completed');

		// 3. Model automatically available for inference
		const prediction = await makePrediction({
			modelId: 'e2e-test',
			features: { x: 1, y: 2 },
		});
		expect(prediction.prediction).toBeDefined();

		// 4. Telemetry collected
		const telemetry = await getRecentTelemetry('e2e-test');
		expect(telemetry.length).toBeGreaterThan(0);

		// 5. Trigger retrain
		const retrainResult = await triggerRetrain('e2e-test');
		expect(retrainResult.newVersion).toBe('v2');

		// 6. New version available
		const prediction2 = await makePrediction({
			modelId: 'e2e-test',
			features: { x: 1, y: 2 },
		});
		expect(prediction2.modelVersion).toBe('v2');
	});
});
```

---

### Implementation Order (TDD Increments)

#### Sprint 1: Data Foundation

1. Write test for FeatureStore.writeFeatures()
2. Implement: Harper table + write method
3. Write test for FeatureStore.getFeatures()
4. Implement: Read method with latest-only logic

#### Sprint 2: Training Core

1. Write test for TrainingProvider.train() (simple binary classifier)
2. Implement: Load data, train TensorFlow.js model
3. Write test for ModelRegistry.registerModel()
4. Implement: Save model metadata to Harper table

#### Sprint 3: Inference & Monitoring

1. Write test for InferenceEngine.loadFromRegistry()
2. Implement: Load model by ID from registry
3. Write test for MonitoringBackend.recordInference()
4. Implement: Write inference event to Harper table

#### Sprint 4: End-to-End Integration

1. Write test for complete flow (E2E test above)
2. Implement REST API endpoints
3. Wire all components together
4. Create demo script

#### Sprint 5: Manual Retraining

1. Write test for triggerRetrain()
2. Implement: Fetch new data, retrain, register new version
3. Implement: Reload model in inference engine
4. Verify new version used in predictions

---

## Initial Implementation Roadmap

### Phase 1: Core Platform (Walking Skeleton + Basic Features)

**Goal**: Production-ready MLOps platform with Harper-native components

**Deliverables**:

#### Codebase Structure

```
harper-edge-ai-example/
├── src/
│   ├── resources.js                 # REST API (existing)
│   ├── PersonalizationEngine.js     # Inference engine (existing)
│   ├── interfaces/                  # TypeScript interface definitions
│   │   ├── InferenceEngine.ts
│   │   ├── FeatureStore.ts
│   │   ├── TrainingProvider.ts
│   │   ├── ModelRegistry.ts
│   │   ├── MessageBus.ts
│   │   ├── MonitoringBackend.ts
│   │   └── DataSource.ts
│   ├── core/                        # Default Harper-native implementations
│   │   ├── FeatureStore.js
│   │   ├── TrainingProvider.js
│   │   ├── ModelRegistry.js
│   │   ├── MessageBus.js
│   │   ├── MonitoringBackend.js
│   │   └── DriftDetection.js
│   ├── adapters/                    # External service adapters (empty in Phase 1)
│   │   └── README.md                # Instructions for implementing adapters
│   └── pipelines/                   # Orchestration logic
│       ├── TrainingPipeline.js
│       └── DeploymentPipeline.js
├── config/
│   ├── retraining-config.yaml       # Retraining trigger configuration
│   └── deployment-config.yaml       # Deployment strategy configuration
├── docs/
│   └── design-2025-11-24.md  # This document
└── examples/
    ├── basic-training.js            # Example: Train a model
    ├── deploy-model.js              # Example: Deploy to edge
    └── monitor-drift.js             # Example: Set up drift monitoring
```

#### Working End-to-End Flow

1. Upload training data to Harper (Data Sources)
2. Engineer features and store in Feature Store
3. Train model using Training Provider
4. Validate and register in Model Registry
5. Deploy to edge locations via CI/CD Pipeline
6. Edge nodes serve predictions via Inference Engine
7. Monitoring collects telemetry and detects drift
8. Retraining triggered when drift exceeds threshold

---

### Phase 2: External Integrations & Advanced Features

**External Adapters**:

- `BigQueryDataSource` - ETL from BigQuery
- `VertexAIEngine` - Cloud inference endpoints
- `VertexAIPipelinesAdapter` - Cloud-based training
- `PubSubAdapter` - Event streaming bridge
- `PrometheusAdapter` - Metrics export

**Advanced Monitoring** (Full Suite D):

- Concept drift detection
- Data quality monitoring
- A/B testing framework
- Model explainability tracking
- Bias & fairness monitoring

**Enhanced Deployment**:

- Blue/Green deployments
- Automatic rollback on error rate increase
- Canary analysis with statistical testing

---

### Phase 3: Five Key Patterns Integration

Once Phase 1 and 2 are stable, implement the five key patterns:

#### Pattern 1: Edge Event Mesh → Real-Time Inference

- Implement device protocols (MQTT, WebSocket)
- Harper Plexus → Pub/Sub bridge
- Dataflow integration for event transformation
- Predictions cached back to Harper

#### Pattern 2: Operational Feature & Prediction Store

- Harper as geo-distributed feature store
- Sync with Vertex AI Feature Store
- Ultra-low-latency edge serving

#### Pattern 3: Event-Driven MLOps

- Harper change events → Pub/Sub
- Cloud Functions trigger Vertex AI Pipelines
- Pipeline logs routed back to Harper

#### Pattern 4: AI Observability & Telemetry

- Vertex AI logs → Pub/Sub → Harper
- Regional dashboards and compliance views
- Real-time observability across all regions

#### Pattern 5: Edge + Cloud Dual Inference (NEW)

**Purpose**: Achieve both speed and accuracy by running inference at edge and cloud simultaneously

**How it works**:

- Send inference request to both edge (fast) and cloud (accurate) in parallel
- Return edge prediction immediately (low latency)
- Use cloud prediction if it arrives within timeout window
- Compare predictions for drift detection
- Smart routing based on:
  - Request complexity (simple → edge, complex → cloud)
  - Cost optimization (minimize cloud calls)
  - Confidence thresholds (low edge confidence → wait for cloud)

**Implementation**:

```typescript
interface DualInferenceConfig {
	edgeEngine: InferenceEngine;
	cloudEngine: InferenceEngine;
	cloudTimeout: number; // Max wait time for cloud response (ms)
	strategy: 'edge_first' | 'cloud_fallback' | 'best_of_both';
}

async function predictWithDualInference(features: FeatureVector, config: DualInferenceConfig): Promise<Prediction> {
	// Start both inferences in parallel
	const edgePromise = config.edgeEngine.predict(features);
	const cloudPromise = config.cloudEngine.predict(features);

	// Wait for edge result
	const edgeResult = await edgePromise;

	// Strategy: Edge first (return immediately)
	if (config.strategy === 'edge_first') {
		return edgeResult;
	}

	// Strategy: Wait for cloud with timeout
	try {
		const cloudResult = await Promise.race([cloudPromise, timeout(config.cloudTimeout)]);

		// Use cloud result if confidence is higher
		if (cloudResult.confidence > edgeResult.confidence) {
			return { ...cloudResult, source: 'cloud' };
		}
	} catch (timeoutError) {
		// Cloud didn't respond in time, use edge
	}

	return { ...edgeResult, source: 'edge' };
}
```

**Use cases**:

- High-stakes predictions: Financial fraud, medical diagnosis
- Variable complexity: Simple cases at edge, complex at cloud
- Quality assurance: Compare edge vs cloud for drift detection

---

## Implementation Approach

1. **Review Design**: Validate architectural decisions and component interfaces
2. **Development Environment**: Configure Harper, TensorFlow.js, and testing framework
3. **Walking Skeleton**: Implement minimal end-to-end flow using TDD
4. **Iterative Development**: Build incrementally with continuous testing
5. **Pattern Integration**: Add the five key patterns once core platform validates

---

## Appendix: Pattern 5 Details (Edge + Cloud Dual Inference)

### Decision Logic

```typescript
interface InferenceDecision {
	useEdge: boolean;
	useCloud: boolean;
	waitForCloud: boolean;
	reason: string;
}

function decideDualInferenceStrategy(
	request: InferenceRequest,
	edgeMetrics: EngineMetrics,
	config: DualInferenceConfig
): InferenceDecision {
	// Always use edge (fast path)
	const decision: InferenceDecision = {
		useEdge: true,
		useCloud: false,
		waitForCloud: false,
		reason: 'edge_only',
	};

	// Trigger cloud inference if:

	// 1. Request complexity high
	if (request.complexity === 'high') {
		decision.useCloud = true;
		decision.waitForCloud = true;
		decision.reason = 'high_complexity';
		return decision;
	}

	// 2. Recent edge confidence low
	if (edgeMetrics.recentAvgConfidence < 0.7) {
		decision.useCloud = true;
		decision.waitForCloud = true;
		decision.reason = 'low_edge_confidence';
		return decision;
	}

	// 3. Quality assurance sampling (10% of requests)
	if (Math.random() < 0.1) {
		decision.useCloud = true;
		decision.waitForCloud = false; // Don't delay response
		decision.reason = 'quality_assurance_sample';
		return decision;
	}

	// 4. User explicitly requests high accuracy
	if (request.requireHighAccuracy) {
		decision.useCloud = true;
		decision.waitForCloud = true;
		decision.reason = 'high_accuracy_requested';
		return decision;
	}

	return decision;
}
```

### Cost Optimization

```typescript
interface CostMetrics {
	edgeCostPerInference: number; // ~$0.00001
	cloudCostPerInference: number; // ~$0.001
	cloudCallsToday: number;
	dailyBudget: number;
}

function shouldUseCloud(decision: InferenceDecision, costMetrics: CostMetrics): boolean {
	if (!decision.useCloud) {
		return false;
	}

	// Check budget
	const projectedCost = costMetrics.cloudCallsToday * costMetrics.cloudCostPerInference;
	if (projectedCost > costMetrics.dailyBudget) {
		console.log('Cloud budget exceeded, using edge only');
		return false;
	}

	return true;
}
```

### Drift Detection via Comparison

```typescript
interface PredictionComparison {
	edgePrediction: Prediction;
	cloudPrediction: Prediction;
	agreement: boolean;
	confidenceDelta: number;
	disagreementReason?: string;
}

async function comparePredictions(edge: Prediction, cloud: Prediction): Promise<PredictionComparison> {
	const agreement = edge.prediction === cloud.prediction;
	const confidenceDelta = Math.abs(edge.confidence - cloud.confidence);

	let disagreementReason: string | undefined;
	if (!agreement) {
		if (confidenceDelta < 0.1) {
			disagreementReason = 'borderline_case';
		} else {
			disagreementReason = 'significant_disagreement';
		}
	}

	const comparison: PredictionComparison = {
		edgePrediction: edge,
		cloudPrediction: cloud,
		agreement,
		confidenceDelta,
		disagreementReason,
	};

	// Track disagreements for drift detection
	if (!agreement && confidenceDelta > 0.2) {
		await trackModelDisagreement(comparison);
	}

	return comparison;
}
```

This pattern provides the best of both worlds: edge speed with cloud accuracy when needed.

---

**End of Design Document**
